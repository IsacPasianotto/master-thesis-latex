\chapter*{Conclusion}\label{chap:concl}
% Do not edit
\addcontentsline{toc}{chapter}{Conclusion}
\label{chap:concl}
\markboth{Conclusion}{Conclusion}

This thesis aimed to investigate the feasibility of migrating HPC workloads to a
cloud environment.
This transition is of high interest because it could offer substantial benefits
to the end users of the cluster and system administrators.
We have seen how, compared to traditional HPC clusters managed by resource
managers like SLURM, cloud environments can offer several advantages: the native
portability of workloads that can be quickly and easily moved from one cluster
to another or a different service provider, thanks to a standardized API and the
use of containers, is a huge advantage that can not be underestimated,
especially in the research field where the concept of reproducibility is
fundamental.
Another advantage is the presence of native environments for data science and
machine learning that can meet the users' needs and provide a more user-friendly
experience like \textit{Jupyter Hub} that can natively interact with tools like
Dask, presented in section \ref{sec:daskcloud}.
Furthermore, cloud solutions tend to offer higher resource management elasticity
and enhanced flexibility in software deployment.
In this document, we develop a proof of concept environment, demonstrating with
tests and quantitative results that this transition is not unrealistic but
rather feasible and achievable in practice.

The benchmarks and test results conducted throughout this research have shown
that moving HPC workloads to the cloud is becoming increasingly viable since
advancements in technology and software are gradually mitigating the
limitations.
New container technologies that have replaced the virtual machines approach are
made so that the computation performances are equal to the ones of executions on
a bare-metal machine.
The limitation that still remains is the network one, and we found that the drop
in network performance is still a critical factor and represents the main
bottleneck in many scenarios.

In particular, a sensitive increase in latency or a lower bandwidth can heavily
affect the overall performance of any network-bound application.
Recent advancements in networking technologies, particularly in the CNI plugins
sector, have sensibly mitigated the issue.
However, much work still needs to be done to overcome this limitation completely.

Knowing that due to the nature of the cloud and the intrinsic structure with
which an orchestrator works (i.e., isolation of the pods' network namespace), it
is impossible to reach the same performance of a traditional HPC cluster, the
hope for the future is to reach a point where that difference becomes
negligible.
In this context, it must be underlined that the results obtained are the result
of measurements conducted, keeping the CNI plugin configurations as close to the
default as possible to try to mimic the situation where a practitioner deploys a
cloud-focused on services and features rather than performance.
It is reasonable to expect a significant improvement if CNIs are properly tuned
and configured to better fit the specific hardware peculiarities and user
exigences they serve.

To conclude, the results of this thesis show that for many situations,
Kubernetes clusters are a valid alternative to the traditional HPC cluster.
The times are not yet mature enough to completely replace the traditional HPC
structure, as there are still performance limitations when multi-node workloads
depend strongly on communication, but a lot of effort is being invested in
addressing those issues.
Until they are solved, in scenarios where performance is crucial and more
important than improvements in the user experience and infrastructure
flexibility, traditional HPC clusters continue to guarantee more satisfactory
results.
In the meantime, it is important to follow the evolution of cloud technologies
and monitor their improvements by continuously testing their performance.

\section*{Possible improvments}\label{sec:improvements}
\addcontentsline{toc}{section}{Possible improvements}

Although many satisfactory results have already been obtained, there are still
plenty of possible paths to try to exploit the underlying hardware as much as
possible to make the SaaS solution even more appealing and competitive.
In particular, we believe that two approaches need to be investigated: the
\textit{Multus CNI} and the enabling of the Infiniband inside containers.

The quite recent but promising project called \textit{Multus CNI}
\cite{multus-presentation, multus-redhat} has gained much attention in the
Kubernetes community, and this is due to its main peculiarity.
What makes Multus so interesting is that it acts as a meta-plugin for
Kubernetes, attaching multiple virtual ethernet interfaces to every pod, each
manageable by a different CNI plugin.
This can be a game-changer in the network in cloud infrastructure for at least
two distinct reasons.
First, this configuration allows the split of the network traffic into multiple
interfaces, each with its own configuration and rules.
An application that shows how to take advantage of this feature is the
possibility of separating all the traffic related to the container orchestrator,
such as the health checks and readiness probes, from the actual communication
the pod must perform to solve its duty.
This can be useful to avoid the network traffic caused by the orchestrator,
allowing the underlying hardware to be fully exploited in cases where more than
one network interface is present in each node.
The second reason why Multus can be a game-changer is the possibility of
choosing which CNI plugin to use for each pod among the available ones.
This can be useful to have more fine-grained control over the network
configuration since results have shown that there is no absolute best CNI among
all the possible situations; e.g., remember that Cilium outperforms all the other
in both the latency and bandwidth tests, but only for small message sizes, while
in the complementary case, Calico and Flannel bested it.
Letting the conscious user free to pick the best CNI for his specific use case
can be a considerable advantage and can lead to more efficient use of the resources.

The other direction of study worth investigating and analyzing is the
possibility of exploiting the Infiniband (IB) network that is present in the
ORFEO cluster.
IB is a high-speed network technology that is widely used in the HPC world, and
it is known for its low latency and high bandwidth.
To exploit the IB performances, it is necessary to use a specific CNI plugin
that can use that technology, such as the Mellanox CNI plugin.
This plugin was developed by Mellanox, a company that is a leader in IB
technology, and was acquired by NVIDIA in 2020.
That CNI plugin promises to be able to exploit the Infiniband network to
guarantee the best possible performance in terms of latency and bandwidth.
This aspect, if confirmed by benchmarks and assessments that must be done, can
be a huge step forward in mitigating the network bottleneck since the Infiniband
network can overcome the ethernet performance by one or two orders of magnitude
in both the latency and the bandwidth.

The author acknowledge the AREA Science Park supercomputing platform ORFEO made
available for conducting the research reported in this thesis and the technical
support of the Laboratory of Data Engineering staff.
