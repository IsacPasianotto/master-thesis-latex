\chapter{Introduction}

% \section{Cloud computing: the actual checkpoint of computing paradigm}

Since the advent of the first computers, the scientific academy has played a
pivotal role in driving progress, with \textit{"computing"} at its core. This
term, interpretable as "the use of available computing resources to solve a
goal-oriented task" has been the bedrock of our technological evolution.
From this perspective, cloud computing is only the current state of an
evolutionary process driven by technological improvements and user requirements;
started many years ago with centralized computing and passed through distributed
and grid computing to arrive at the current state of on-demand resources
\cite{Surbiryala2019CloudCH}.

\section{What is cloud computing}

Giving a precise and formal definition of "cloud computing" is not so
straightforward because, as R.Buya \emph{et al.}\cite{RBuyya2011}:
\textit{"Cloud computing has been coined as an umbrella term to describe a category of
sophisticated on-demand computing services initially offered by commercial providers"}.

In general, the scientific literature agrees that cloud computing is an approach
technology in which the end user does not need to configure (or even know) all
the implementation details that make it possible to use the resources that this
requires. This paradigm makes it possible to minimize the effort needed to have
a functioning environment, focusing mainly on the actual usage of it.

The standard proposed by the National Institute of Standards and Technology
(NIST)\cite{nistdef} outlines the characteristics that a "cloud" solution must
present to be defined as such. It also defines two widely used categorizations
to classify the different solutions available.


% The definition proposed by the National Institute of Standards and Technology
% \cite{nistdef} is a milestone in the sector's literature that has to be quoted.
% This document outlines the main characteristics that a "cloud" solution
% presents, as well as two widely used categorizations to classify the different
% solutions available.

According to NIST, the essential characteristics to ensure that we can
talk about cloud computing are: "\textit{On-demand self-service}",
in the sense that a consumer can unilaterally receive computing capabilities
without direct interaction with the service provider, "\textit{Broad network
  access"}, given that access to these resources occurs via the Internet.
 Also listed among the essential characteristics outlined are "\textit{Resource
  pooling}",
i.e. the fact that computational resources are given to users in a multi-tenant
model and can be dynamically assigned and re-assigned to accommodate user
requests.
These changes occur quickly, sometimes even automatically, and to the end user,
the available resources it can request appear virtually unlimited.
Finally, each cloud system integrates automatic systems to control and optimize
the use of resources - "\textit{Measured service}".

Speaking instead of how to categorize a cloud service, the first distinction we
can make is that between \textit{SaaS}, \textit{PaaS} and \textit{Iaas}, which
respectively mean Software as a Service, Platform as a Service and
Infrastructure as a Service. What differentiates them is how the amount of
effort and the possibility of controlling all the tasks necessary to achieve a
ready-to-use system are distributed between the customer and the provider.

\noindent Furthermore, depending on who owns the hardware resources to install the cloud
infrastructure, the following categories can be delineated: "\textit{Private
  cloud}", "\textit{Community cloud}" "\textit{Public cloud}" and
"\textit{Hybrid cloud}".

\section{Literature review}\label{sec:intro-literature-review}

The cloud computing concept is relatively new, as is its definition, and his
paradigm finds its roots in modern principles and technologies. Different needs
often drive the approaches adopted in the cloud compared to those used in HPC,
so almost 15 years after its formalization, only some HPC workloads have been
transitioned. While making such a transition is not mandatory, those who
overtook this path found themselves in a better situation after reaching
convergence with more portable codes and lower infrastructure costs by scaling
the computational resources adaptively to meet fluctuating workload demands.
Nevertheless, before taking this path, the trade-offs between the costs and
benefits and the feasibility of such a move should be assessed.

% This entire thesis aims to find an answer to the question \textit{"Is
%   it possible to move HPC applications to a cloud environment?"}.
% Doing such a transition is not mandatory, but the cloud computing paradigm finds
% its roots in more modern principles and technologies than those used in HPC and
% the assessment of the trade-offs between the costs and benefits of such a move
% should be performed.

% The scientific literature has already faced this question, particularly at the beginning of the 2010s when the topic started to catch the community's attention. In particular, some of the first works worthy to mention are the ones by D. Ghoshal \emph{et al.}~\cite{Ghoshal2011} and  R. Exp\'osito \emph{et al.}~\cite{exposito2013}.  The authors were quite skeptical about performing that transition in all the cases. Many critical points were raised, primarily focused on the performance.  Ghoshal  pointed out how the the I/O operations quickly became the system's bottleneck and how the performance measurements were affected by an incredibly high variance.  A more structured approach to evaluate the performance of HPC applications in the cloud is proposed by Exp\'osito with his team.  They performed several scaling tests of MPI codes (varying the MPI implementation) over the Amazon EC2 cloud. The results were not encouraging, as the results show that the network latency affected the application's overall performance.  Therefore, it is not surprising that in that period, the general sentiment about that the transition of the HPC in the cloud was quite negative. One of the first voices out of the choir was the one of Gupta \cite{Gupta2013}, who pointed out in 2013 that it could be some specific cases where the cloud is a proper alternative (and maybe cheaper) concerning a traditional HPC infrastructure, stating also how, with the years, possible improvements could alter the situation.

The scientific literature started to address the cloud transition at the
beginning of the 2010s with the advent of several affordable and on-demand
commercial cloud solutions. D. Ghoshal \emph{et al.}~\cite{Ghoshal2011} and  R.
Exp\'osito \emph{et al.}~\cite{exposito2013} were among the first to explore the
possibility of moving their HPC workflows to the cloud. Unfortunately, their
preliminary results were unfavorable, leaving them skeptical about performing
that transition. In particular, multiple performance shortcomings were found.
Ghoshal pointed out how the the I/O operations quickly became the system's
bottleneck and how the performance measurements were affected by an incredibly
high variance. Exp\'osito and his team then proposed a more structured approach
to evaluating the performance of HPC applications in the cloud: they performed
several scaling tests of MPI codes (varying the MPI implementation) over the
Elastic Compute Cloud (EC2). These results showed that the complicated network
setup induced a latency that affected the application's overall performance.
Unsurprisingly, in that period, the general sentiment about the transition of
the HPC in the cloud was quite negative. Nevertheless, economic arguments have
been made that Small and medium-scale organizations could obtain better
performance/cost ratios in the cloud than in on-premises infrastructures,
envisioning a future where the improvement in the software responsible for the
virtualization would lead to the popularization of the novel solution
\cite{Gupta2013}.


% After that period, the enthusiasm for the possibility of this interesting
% workload transition has decreased, and after a decade, the scientific community
% has yet to focus too much on this topic. However, in recent times, XXfor some
% reasonXX some works have started to re-evaluate the possibility, like the one by
% Saini \emph{et al.}~\cite{Saini2021} (2021) and Reed \emph{et
% al.}~\cite{Reed2022} (2022). Why is it interesting to re-evaluate this
% possibility? The main reason is that the technologies have evolved a lot, and
% some results that were found are not guaranteed to hold anymore. One particular
% aspect that we can explain in detail in chapter \ref{sec:chpt1-containers} why
% is so crucial, is the fact that all the 2010s work taken into account only the
% VMs and not the containers which has become more and more adopted year after
% year. Both the works by Saini and Reed in facts conclude with optimistic notes
% about the evolution of cloud vendors' infrastructure and the always-increasing
% performance of the cloud resources can exhibit. Obviously, the transition is not
% painless, and some critical points are still present, like the network latency
% problem, and some authors do not share the same enthusiasm. Just to mention a
% very recent work, Keke \cite{Keke2023} (2023) remarked how solving a cloud-only
% problem: The security of the data can slow down the overall processes.
% Unfortunately, since in the cloud computing scenario typically the data is
% stored in a remote location, not managed by the owner of the data, this price
% has to be paid.

After the initial momentum, the enthusiasm for the possibility of this
interesting transition to the cloud faded. For the next decade, the scientific
community focused only a little on the topic. However, in the last few years,
thanks to the aggressive pricing of cloud solutions and the changes in the
science computational workflows, works like Reed \emph{et al.}~\cite{Reed2022}
and like Saini \emph{et al.}~\cite{Saini2021} started to re-evaluate the
possibility of a change in the paradigm. Why is it interesting to re-evaluate
this possibility? The main reason is that the technologies have evolved, and
some found results are not guaranteed to hold anymore\cite{Guidi_2021}. One of
the main aspects that will be better detailed in chapter
\ref{sec:chpt1-containers} is that all the works in the years 2010s approach the
cloud only taking into account the Virtual Machine (VM) strategy for
virtualization, not considering containers that have become the most adopted
technique to deploy cloud infrastructure. Despite the positive conclusion drawn
by the recent literature, it comes as no surprise that the transition is not
painless, and some critical points are still present. For example, the network
latency problem underwent several improvements from hardare\cite{Firestone_2018}
to software [cite ebpf]. Still, as we will show in this work, it is not at par
with standard HPC equivalent environments. Furthermore, new problems, such as
managing secure and confidential data that needs to be uploaded on the cloud to
be processed, are arising \cite{Keke2023}.

\section{Aim statement}

As mentioned, the principal ambition that inspired this work is to understand if
it is possible to move HPC applications and workloads to a cloud environment.
Regarding what was said in the previous section, one crucial aspect must
be precisely specified...

Virtually all the studies that have been conducted, or at least
the very great majority that authors have been able to find have been performed
on what is defined according to the NIST categorization as a Public Cloud.
Most of those findable resources drew their conclusion on benchmarks and
measurements performed on some on-demand resources provided by the most famous
cloud vendors, in particular Amazon Web Services (AWS), the most used one
Instead, the solutions analyzed in the following pages fall under the category
of the Private Cloud.
This distinction is not trivial since physically owning the hardware resources
on which the Cloud-compliant environment is installed implies having the
possibility (but also the duty) to manage aspects that in a Public Cloud are
completely hidden or at least out of the user's control.
This double-edged sword indeed implies an increase in the complexity of the
system, but with this higher level of freedom, it is possible to tailor the
system to the specific needs of the user, and this can (hopefully) lead to
better performance and a more efficient use of the resources.

In this specific case, the hope in spending time and resources to build such
infrastructure is, eventually, to let the clusters be usable by its users in
a SaaS fashion. This means letting the user focus only on their code without
worrying about the infrastructure, the software stack, and the system's
configuration.
It is also worth mentioning that, at least for the ORFEO users (ORFEO is the
cluster that is the subject of this work), the typical use case is the execution
of some interpreted (mostly Python and R) scripts which, unlike the old compiled
language, may need additional effort other that the coding process itself to
exploit the full potential of the hardware resources, and it is not guaranteed
that some tricks used to do that still works in a different cluster.
Instead, as it will be shown, being more recent, cloud solutions tend to be
tuned to the needs of the more recently adopted languages and technologies.
This will be a key point in evaluating the solution proposed in this work since
the user experience and time spent on adaptations is a heavy factors to consider
in a final trade-off evaluation.

Besides this, since the cloud computing paradigm is an extensive topic, it is
growing increasingly daily. Technologies that a few years ago were just in
the embryonic stage (like containers and container orchestrators) are now the
standard. For this reason, it is possible to consider this work as a periodical
review of the state of cloud technologies and how far they have come.




% Regarding the categorizations just mentioned in the previous section, the
% solution discussed in this composition is an example of SaaS in a Private Cloud
% context.





